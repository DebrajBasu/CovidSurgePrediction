{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import pacf \n",
    "from statsmodels.tsa.stattools import acf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the autocorrelation and partial autocorrelation plots to find the number of lags in the time series that we will use to validate the time series model and finally build the testing case\n",
    "Autocorrelation and partial autocorrelation plots are heavily used in time series analysis and forecasting.These are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n",
    "\n",
    "We can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation.\n",
    "\n",
    "A plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a correlogram or an autocorrelation plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Updated_CovidDataset.pkl')\n",
    "#values = series.values\n",
    "## Only values from \n",
    "from datetime import datetime\n",
    "df['INDEX_DATE'] = df['INDEX_DATE'].astype('datetime64[ns]')\n",
    "datetime_str = '11-30-20'\n",
    "datetime_object = datetime.strptime(datetime_str, '%m-%d-%y')\n",
    "df_validation = df[df['INDEX_DATE'] <= datetime_object ].reset_index(drop= True)\n",
    "data =df_validation['D1_COVID_NEW_ADM_CNT'].values\n",
    "# # data =df[Feature_set + ['D1_COVID_NEW_ADM_CNT']].values\n",
    "# data = data.reshape(-1,1)\n",
    "\n",
    "# series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0)\n",
    "\n",
    "plot_acf(data, lags=5)\n",
    "pyplot.xlabel('lags')\n",
    "pyplot.ylabel('Autocorrelation')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partial autocorrelation is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n",
    "\n",
    "The blue shadow within the plot showing the confidence interval is relative and signifies which one is more statistically significant than the other.\n",
    "\n",
    "- stattools.pacf computes the confidence interval around the estimated pacf, i.e. it's centered at the actual value\n",
    "- graphics.tsa.plot_pacf takes that confidence interval and subtracts the estimated pacf, So the confidence interval is centered at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(data, lags=5, alpha=.05)\n",
    "pyplot.xlabel('lags')\n",
    "pyplot.ylabel('Partial Autocorrelation')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B =pacf(data, nlags=5, alpha=.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we doing with PACF: In practical application of AR modelling, it is important to know how much information from the past (lag) we can reliably use to convert the time series into a supervised learning model AND how often in the future it should run (like every day, every fifth day etc.) to update the model. Since we are dealing with a NON-STATIONARY signal and we are not sure of the trend and seasonality, we cannot use a fixed model to run indefinitely.\n",
    "\n",
    "Looking at the plot of partial autocorrelation, we can see that a lag value of 3 can be used for training the model to start with i.e. 3 past values from the past with the current value can be used to build the training data set for the supervised learning model. \n",
    "\n",
    "#### Designing the walk forward validation \n",
    "rolling forward corss validation to assess the behaviors of the different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast monthly births with random forest\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "# transform a time series dataset into a supervised learning dataset\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = (1 if type(data) is list else data.shape[1])\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an random forest model and make a one step prediction\n",
    "def random_forest_forecast(train, testX):\n",
    "    # transform list into array\n",
    "    train = asarray(train)\n",
    "    # split into input and output columns\n",
    "    trainX, trainy = train[:, :-1], train[:, -1]\n",
    "    \n",
    "    # fit model\n",
    "    model = RandomForestRegressor(n_estimators=1000)\n",
    "#     model = GradientBoostingRegressor(n_estimators=250, max_depth=3,\n",
    "#                                 learning_rate=.1, min_samples_leaf=9,\n",
    "#                                 min_samples_split=9)\n",
    "#     #model = LinearRegression()\n",
    "#     trainX = scaler.fit_transform(trainX)\n",
    "# #     testX = testX.reshape(-1,1)\n",
    "#     print(testX)\n",
    "#     testX = scaler.transform([testX])\n",
    "    model.fit(trainX, trainy)\n",
    "    # make a one-step prediction\n",
    "    \n",
    "    yhat = model.predict([testX])\n",
    "    return yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    l, test = train_test_split(data, n_test)\n",
    "#     print(train)\n",
    "#     print(test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        \n",
    "        # fit model on history and make a prediction\n",
    "        yhat = random_forest_forecast(history, testX)\n",
    "        yhat = np.ceil(yhat)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "#         print(test[i])\n",
    "        history.append(test[i])\n",
    "        history = history[1:]\n",
    "        print(len(history))\n",
    "        # summarize progress\n",
    "        print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    \n",
    "    error = mean_absolute_error(test[:, -1], predictions)\n",
    "    return error, test[:, -1], predictions\n",
    "\n",
    "df = pd.read_pickle('Updated_CovidDataset.pkl')\n",
    "#values = series.values\n",
    "## Only values from \n",
    "from datetime import datetime\n",
    "df['INDEX_DATE'] = df['INDEX_DATE'].astype('datetime64[ns]')\n",
    "datetime_str = '11-30-20'\n",
    "datetime_object = datetime.strptime(datetime_str, '%m-%d-%y')\n",
    "df_validation = df[df['INDEX_DATE'] <= datetime_object ].reset_index(drop= True)\n",
    "data =df_validation['D1_COVID_NEW_ADM_CNT'].values\n",
    "data = data.reshape(-1,1)\n",
    "# transform the time series data into supervised learning\n",
    "\n",
    "data = series_to_supervised(data, n_in=2)\n",
    "# Add the other features which may be useful for prediction\n",
    "# evaluate\n",
    "mae, y, yhat = walk_forward_validation(data, 30)\n",
    "yhat =np.ceil(np.array(yhat))\n",
    "print('MAE: %.3f' % mae)\n",
    "# plot expected vs predicted\n",
    "pyplot.figure(figsize=(10,8))\n",
    "pyplot.plot(y, '--ob',label='Expected')\n",
    "pyplot.plot(yhat, '-xr',label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment scenario that is realistic\n",
    "Every day mid night we receive the number from the last day and use that to retrain the new model and predict the next day numbers\n",
    "### What kind of penalty or performance metrices are we looking at?\n",
    "- Penalty for underprediction ?\n",
    "- No penalty for over prediction ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating and testing a deployment scenario\n",
    "Now that we have validated our model till 11-30-2020, we are going to use the model to predict the next day but use the same rolling walk forward method, where we use the past data to predict the future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "from datetime import datetime as dt, timedelta\n",
    "df = pd.read_pickle('Updated_CovidDataset.pkl')\n",
    "df['INDEX_DATE'] = df['INDEX_DATE'].astype('datetime64[ns]')\n",
    "datetime_str_current = '12-1-20'\n",
    "date_back =3\n",
    "\n",
    "datetime_object = dt.strptime(datetime_str, '%m-%d-%y')\n",
    "df_test = df[df['INDEX_DATE'] >= datetime_object ].reset_index(drop= True)\n",
    "data_test =df_test['D1_COVID_NEW_ADM_CNT'].values\n",
    "# # data =df[Feature_set + ['D1_COVID_NEW_ADM_CNT']].values\n",
    "#data = data.reshape(-1,1)\n",
    "\n",
    "# # transform the time series data into supervised learning\n",
    "## ------------------------------------------------\n",
    "datetime_str = '11-30-20'\n",
    "datetime_object = datetime.strptime(datetime_str, '%m-%d-%y')\n",
    "df_validation = df[df['INDEX_DATE'] <= datetime_object ].reset_index(drop= True)\n",
    "data =df_validation['D2_COVID_NEW_ADM_CNT'].values\n",
    "#data =(df_validation['COVID_MED_SURG_NO_HFNC_PCT_M'].values)/100\n",
    "\n",
    "data = data.reshape(-1,1)\n",
    "\n",
    "train_current = series_to_supervised(data, n_in=4)\n",
    "# split into input and output columns\n",
    "trainX, trainy = train_current[:, :-1], train_current[:, -1]\n",
    "# # fit model\n",
    "model = GradientBoostingRegressor(n_estimators=250, max_depth=3,\n",
    "                                learning_rate=.1, min_samples_leaf=9,\n",
    "                                min_samples_split=9)\n",
    "model.fit(trainX, trainy)\n",
    "##------------------------------------------------------\n",
    "# # construct an input for a new prediction\n",
    "# row = data[-3:].flatten()\n",
    "row  = data[-4:].flatten()\n",
    "\n",
    "# # make a one-step prediction\n",
    "yhat = model.predict(asarray([row]))\n",
    "print('Input: %s, Predicted: %.3f' % (row, yhat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime as dt, timedelta\n",
    "df = pd.read_pickle('Updated_CovidDataset.pkl')\n",
    "df['INDEX_DATE'] = df['INDEX_DATE'].astype('datetime64[ns]')\n",
    "datetime_str_current = '12-1-20'\n",
    "\n",
    "datetime_object = dt.strptime(datetime_str, '%m-%d-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime as dt, timedelta\n",
    "days = datetime.timedelta(5)\n",
    "\n",
    "new_date = datetime_object - days\n",
    "new_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
